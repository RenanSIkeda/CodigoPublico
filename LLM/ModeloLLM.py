import pandas as pd
from transformers import pipeline

# -----------------------------
# 1. Create a sample budget dataset
# -----------------------------
data = pd.DataFrame({
    "Department": ["IT", "Finance", "HR", "Sales", "Operations"],
    "Previous_Budget": [120000, 90000, 70000, 150000, 200000],
    "Current_Spending": [135000, 87000, 65000, 162000, 210000],
    "Active_Projects": [5, 3, 2, 6, 8],
    "Employees": [12, 8, 6, 15, 20]
})

# Calculate budget variation
data["Variation_%"] = ((data["Current_Spending"] - data["Previous_Budget"]) / data["Previous_Budget"]) * 100

print("ğŸ“Š Budget Dataset:\n")
print(data)

# -----------------------------
# 2. Load local lightweight LLM (CPU-based)
# -----------------------------
print("\nâ³ Loading local model...")

# âš™ï¸ Aqui vocÃª pode trocar o nome do modelo abaixo para outras opÃ§Ãµes recomendadas:
# -------------------------------------------------------------------------------
# ğŸ§© Modelos LEVES (rodam em CPU com 8 GB RAM):
# "distilgpt2"                        â†’ Muito leve (~300 MB), rÃ¡pido, mas respostas simples.
# "google/flan-t5-small"              â†’ Segue instruÃ§Ãµes bÃ¡sicas (melhor que GPT-2).
# "google/flan-t5-base"               â†’ Boa qualidade e compreensÃ£o de tarefas.
# "google/gemma-2b-it"                â†’ Novo modelo instruÃ­do da Google (bom equilÃ­brio).
# "pierreguillou/gpt2-small-portuguese" â†’ VersÃ£o GPT-2 treinada em portuguÃªs.

# ğŸš€ Modelos INTERMEDIÃRIOS (melhor texto, exigem 12â€“16 GB RAM):
# "tiiuae/falcon-7b-instruct"         â†’ Modelo de 7B instruÃ­do, entende prompts complexos.
# "mistralai/Mistral-7B-Instruct-v0.1" â†’ Gera textos muito coerentes e corporativos.
# "google/flan-t5-large"              â†’ Alta qualidade, mas mais pesado.
# "google/gemma-7b-it"                â†’ Novo, otimizado para instruÃ§Ãµes e anÃ¡lise.

# ğŸ’¼ Modelos AVANÃ‡ADOS (precisam de GPU ou 24+ GB RAM):
# "meta-llama/Llama-3.1-8B-Instruct"  â†’ Excelente qualidade e raciocÃ­nio, ideal para relatÃ³rios.
# "meta-llama/Llama-2-7b-chat-hf"     â†’ Boa alternativa open-source com linguagem natural fluente.
# "NousResearch/Nous-Hermes-2-Mistral-7B" â†’ Ajustado para tarefas de anÃ¡lise corporativa e resposta longa.

model = pipeline(
    "text-generation",
    model="tiiuae/falcon-7b-instruct",  # Troque aqui por qualquer modelo listado acima
    device=-1                           # ForÃ§a execuÃ§Ã£o em CPU
)

# -----------------------------
# 3. Build the analysis prompt
# -----------------------------
summary = data.to_string(index=False)

prompt = f"""
You are a senior financial analyst.
Review the following budget data and write a short executive report.

Budget data:
{summary}

Include:
1. Departments with budget overruns.
2. Areas with strong financial control.
3. Strategic recommendations.
"""

# -----------------------------
# 4. Generate report using local LLM
# -----------------------------
print("\nğŸ§  Generating analysis with local model...\n")
response = model(
    prompt,
    max_new_tokens=300,
    temperature=0.5,
    do_sample=True,
    top_p=0.8
)

# -----------------------------
# 5. Display report
# -----------------------------
generated_text = response[0]["generated_text"].replace(prompt, "").strip()

print("ğŸ§¾ Executive Budget Report:\n")
print(generated_text)
